{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1e19f9a-fd0e-47b2-a4c7-e8523e776b76",
   "metadata": {},
   "source": [
    "# MNE-RSA: representational similarity analysis on source-level MEG data\n",
    "<img src=\"images/adept.png\" width=\"220\" style=\"float: right\">\n",
    "\n",
    "While sensor-level RSA is useful to get a first impression of how neural representations unfold over time, it is not really suited to study differences between brain regions.\n",
    "For this, you want to so RSA in a searchlight pattern across the cortex.\n",
    "\n",
    "The knowledge you have gained from your sensor-level analysis will serve you well for this part, as the API of MNE-RSA is mostly the same across sensor- and source-level analysis.\n",
    "However, performing a searchlight analysis is a heavy computation that takes a lot of time.\n",
    "Since time is limited, I'll be taking the opportunity to show you a bit more about the API regarding restricting the analysis to parts of the data in several ways.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>EXERCISE</b>:\n",
    "    \n",
    "In the cell below, update the `data_path` variable to point to where you have extracted the [`BerlinBrains2023DataPackage.zip`](https://drive.google.com/file/d/1etefiAIRG6CMBeU91Fu2CTqM5KT9Ng_Z/view?usp=drive_link) file to.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ade4a-723c-47bf-9b7e-9b85ca1cd777",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 90  # Tune this to make figures bigger/smaller\n",
    "\n",
    "# Set this to where you've extracted `data.zip` to\n",
    "data_path = \"../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8850b8-2a2a-4cdc-af2e-f147816711ab",
   "metadata": {},
   "source": [
    "We'll start by loading the `epochs` again, but this time, we will restrict them to only two experimental conditions:\n",
    "the first presentations of famous faces versus scrambled faces.\n",
    "This will reduce the number of rows/columns in our RDMs and hence speed up computing and comparing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30325a27-f4cc-466d-8354-456ef99cc38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "epochs = mne.read_epochs(f\"{data_path}/sub-02/sub-02-epo.fif\")\n",
    "epochs = epochs[['face/famous/first', 'scrambled/first']]\n",
    "epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314e6dea-61f8-46fa-9d31-0a8d990c1d44",
   "metadata": {},
   "source": [
    "When we select a subset of epochs, the `epochs.metadata` field is likewise updated to match the new selection.\n",
    "This feature is one of the main reasons to use the `.metadata` field instead of keeping a separate `DataFrame` manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1b784b-8c2c-48d2-8bd8-520ccd3d4080",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs.metadata.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3362a0a-0fe0-4a05-a153-95169b901dc5",
   "metadata": {},
   "source": [
    "This means we can use the `epochs.metadata[\"file\"]` column to restrict the pixel and FaceNet RDMs to just those images still present in the MEG data.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>EXERCISE</b>:\n",
    "    \n",
    "In the cell below, I've already written the code to read the images and FaceNet embeddings, I'll leave it to you to select the proper rows from the data matrices and use `compute_dsm` to compute the appropriate RDMs.\n",
    "\n",
    "(HINT: try a syntax like `\"f123.bmp\" in epochs.metadata[\"file\"].values` to test if a filename is present in the epochs)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732b690c-913d-479f-95cb-05973552f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "files = sorted(glob(f\"{data_path}/stimuli/*.bmp\"))\n",
    "pixels = np.array([np.array(Image.open(f)) for f in files])\n",
    "\n",
    "store = np.load(f\"{data_path}/stimuli/facenet_embeddings.npz\")\n",
    "filenames = store[\"filenames\"]\n",
    "embeddings = store[\"embeddings\"]\n",
    "\n",
    "# Write the code below to select the proper rows from `pixels` and `embeddings`\n",
    "# and compute the RDMs.\n",
    "from mne_rsa import compute_rdm\n",
    "pixel_rdm = # ...\n",
    "facenet_rdm = # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598d4e45-e8a9-46ca-97c4-f85a0601ed87",
   "metadata": {},
   "source": [
    "Executing the cell below will test whether the RDMs have been properly constructed and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d044052-565f-4284-bd6b-4953921d36b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne_rsa import plot_rdms\n",
    "from scipy.spatial.distance import squareform\n",
    "if len(pixel_rdm) != len(facenet_rdm):\n",
    "    print(\"The pixel and FaceNet RDMs are of difference sizes, that can't be right. 🤔\")\n",
    "elif len(pixel_rdm) != 43956:\n",
    "    print(\"Looks like your RDMs do not have the correct rows. 🤔\")\n",
    "elif squareform(pixel_rdm)[:150, :150].mean() >= squareform(pixel_rdm)[150:, 150:].mean():\n",
    "    print(\"The pixels RDM doesn't look quite right. Make sure the rows are in alphabetical filename order. 🤔\")\n",
    "elif squareform(facenet_rdm)[:150, :150].mean() <= squareform(facenet_rdm)[150:, 150:].mean():\n",
    "    print(\"The FaceNet RDM doesn't look quite right. Make sure the rows are in alphabetical filename order. 🤔\")\n",
    "else:\n",
    "    print(\"Your RDMs look just right! 😊\")\n",
    "    plot_rdms([pixel_rdm, facenet_rdm], names=[\"pixels\", \"facenet\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66021813-fdbe-40a8-ba06-e92eaf4c2fd7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>EXERCISE</b>:\n",
    "    \n",
    "As was the case with the sensor-level RSA, we also need the `y` array that assigns a number to each epoch indicating which one of the 297 images was shown, taking care to assign numbers that respect the order in which the images appear in `pixel_dsm` and `facenet_dsm`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2e572e-dc9e-45e0-a0c6-202a3de8e036",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = # write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d42b35-c53c-4092-909d-7709af5ca29a",
   "metadata": {},
   "source": [
    "## To source space!\n",
    "\n",
    "In order to perform RSA in source space, we must create source estimates for the epochs.\n",
    "There's many different ways to do this, for example you'll learn about beamformers during this workshop, but here we're going to use the one that is fastest.\n",
    "If we use MNE, we can use a pre-computed inverse operator and apply it to the epochs to quickly get source estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c961409-15d6-46b5-b3a8-be714737f979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.minimum_norm import read_inverse_operator, apply_inverse_epochs\n",
    "inv = read_inverse_operator(f\"{data_path}/sub-02/sub-02-inv.fif\")\n",
    "stcs = apply_inverse_epochs(epochs, inv, lambda2=1/9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bea7bc-e4d4-4b70-87a2-70e57ab7b0d1",
   "metadata": {},
   "source": [
    "The result is a list of 297 `SourceEstimate` objects. Here are the first 5 of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b138534-733a-45f8-beb4-742e03b1358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stcs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dccacd6-d19f-42f9-b780-870042c30741",
   "metadata": {},
   "source": [
    "<img src=\"images/distances.svg\" style=\"float: right; margin-left: 50px\" width=\"300\">\n",
    "\n",
    "The plan is to perform RSA in a searchlight pattern, not only as a sliding window through time, but also sliding across different locations across the cortex.\n",
    "To this end, we'll define spatial patches with a certain radius, and only source points that fall within a patch are taken into account when computing the RDM for that patch.\n",
    "The cortex is heavily folded and ideally we define distances between source point as the shortest path along the cortex, what is known as the geodesic distance, rather than straight euclidean distance between the XYZ coordinates.\n",
    "MNE-Python is here to help us out in this regard, as it contains a function to compute such distances and store them within the [`SourceSpaces`](https://mne.tools/stable/generated/mne.SourceSpaces.html) object (through the [`add_source_space_distances`](https://mne.tools/stable/generated/mne.add_source_space_distances.html) function).\n",
    "\n",
    "Let's load the file containing the proper source space with pre-computed geodesic distances between source points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317b533-a36b-4996-b59e-02ceef46a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne import read_source_spaces\n",
    "src = read_source_spaces(f\"{data_path}/freesurfer/sub-02/bem/sub-02-oct5-src.fif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf0b991-8fb9-434f-98c8-770e9b3cc6cf",
   "metadata": {},
   "source": [
    "To speed things up, let's restrict the analysis to only the occipital, parietal and temporal areas on the left hemisphere.\n",
    "There are several ways to tell MNE-RSA which source points to use, and one of the most convenient ones is to use [`mne.Label`](https://mne.tools/stable/generated/mne.Label.html) objects.\n",
    "This allows us to define the desired areas using the \"aparc\" atlas that FreeSurfer has constructed for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a3f9eb-b46b-495d-8c39-1d433ac6d3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rois = mne.read_labels_from_annot(\"sub-02\", parc=\"aparc\", subjects_dir=f\"{data_path}/freesurfer\", hemi=\"lh\")\n",
    "\n",
    "# These are the regions we're interested in\n",
    "roi_sel = ['inferiortemporal-lh', 'middletemporal-lh', 'fusiform-lh', 'bankssts-lh', 'inferiorparietal-lh', 'lateraloccipital-lh', 'lingual-lh', 'pericalcarine-lh', 'cuneus-lh', 'supramarginal-lh', 'superiorparietal-lh']\n",
    "rois = [r for r in rois if r.name in roi_sel]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0502fb6-2e62-4797-947a-e947176e2e64",
   "metadata": {},
   "source": [
    "## Source-space RSA\n",
    "\n",
    "Time to actually perform the RSA in source space. The function you need is [`rsa_stcs`](https://users.aalto.fi/~vanvlm1/mne-rsa/functions/mne_rsa.rsa_stcs.html).\n",
    "Take a look at the documentation of that function, which should look familiar is it is very similar to [`rsa_epochs`](https://users.aalto.fi/~vanvlm1/mne-rsa/functions/mne_rsa.rsa_epochs.html) that you have used before.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>EXERCISE</b>:\n",
    "    \n",
    "Ga ahead and call [`rsa_stcs`](https://users.aalto.fi/~vanvlm1/mne-rsa/functions/mne_rsa.rsa_stcs.html) in the cell below.\n",
    "Keep the following in mind:\n",
    " - Perform RSA on the source estimates, using the pixel and FaceNet RDMs as model RDMs.\n",
    " - Store the result in a variable called `stc_rsa`\n",
    " - Searchlight patches should have a spatial radius of 2cm (=0.02 meters).\n",
    " - Searchlight patches should have a temporal radius of 0.05 seconds\n",
    " - Restrict the analysis to  0.0 to 0.5 seconds\n",
    " - Restrict the analysis to just the cortical regions (`rois`) we've selected above\n",
    " - Optionally set `n_jobs=-1` to use all CPU cores\n",
    " - Optionally set `verbose=True` to show a progress bar.\n",
    " \n",
    "Depending on the speed of your computer, this may take anywhere from a few seconds to a few minutes to complete.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ba6782-fce4-4a67-9acc-6bd2bda1a6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne_rsa import rsa_stcs\n",
    "stc_rsa = # write your call to rsa_stcs() here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd762d53-feeb-4bf4-aa6e-a6ce8f3be0ea",
   "metadata": {},
   "source": [
    "If everyting went as planned, executing the cell below will plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3088112-7142-47eb-97c3-57362bd4dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For clarity, only show positive RSA scores\n",
    "stc_rsa[0].data[stc_rsa[0].data < 0] = 0\n",
    "stc_rsa[1].data[stc_rsa[1].data < 0] = 0\n",
    "\n",
    "# Show the RSA maps for both the pixel and FaceNet RDMs\n",
    "brain_pixels = stc_rsa[0].plot(\"sub-02\", subjects_dir=f\"{data_path}/freesurfer\", hemi=\"both\", initial_time=0.081, views=\"ventral\", title=\"pixels\")\n",
    "brain_facenet = stc_rsa[1].plot(\"sub-02\", subjects_dir=f\"{data_path}/freesurfer\", hemi=\"both\", initial_time=0.180, views=\"parietal\", title=\"FaceNet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c71b4ef-54e2-427e-b528-0875ca995c5c",
   "metadata": {},
   "source": [
    "If you've made it this far, you have successfully completed your first sensor-level RSA! 🎉 This is the end of this notebook. In the next notebook, we will discuss group-level analysis and statistics.\n",
    "\n",
    "<center>\n",
    "<a href=\"statistics.ipynb\"><img width=\"200\" src=\"images/expert.png\"></a><br>\n",
    "<a href=\"statistics.ipynb\" style=\"font-size: 20pt\">>>>>> Continue to statistics >>>>></a>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
